# LSA Annual Meeting 2025 Session: Dynamic Field Theory for unifying discrete and continuous aspects of linguistic representations

This is the official GitHub repository for the LSA Annual Meeting 2025 Session: Dynamic Field Theory for unifying discrete and continuous aspects of linguistic reprsentations.

## Introduction
Models of human language processing generally make use of the discrete categories of linguistic theory: e.g., features, phonemes, morphemes, words (Dell, 1986; Levelt, 1999; McClelland & Elman, 1986; Norris, 1994; Norris & McQueen, 2008). The nature of the link between these categories and the continuous dimensions of speech articulation and acoustics remains a frontier in language science (e.g., Goldrick & Chu, 2014, c.f., Pouplier & Goldstein, 2014). A large body of evidence suggests that continuous phonetic patterns can be understood as reflecting gradient activation of discrete linguistic categories (e.g., Wright, 2004; Goldrick & Blumstein, 2006; Pouplier & Goldstein, 2010; Baese-Berk & Goldrick, 2009; Wedel et al., 2018). Recently, a unified perspective on the discrete and continuous aspects of language has been pursued in the framework of Dynamic Field Theory (DFT: Schöner et al., 2016).

DFT is a general framework for understanding action, perception and cognition as arising from the dynamics of neural activity. Neural activity is described using the mathematics of dynamical systems, which has been recognized for its potential to unify discrete and continuous aspects of linguistic representations (Iskarous, 2017; Gafos & Benus, 2006). In DFT, features relevant for cognition are modeled as continuous parameters governed by dynamic neural fields, henceforth DNFs (Amari, 1977). DFT has been applied to a wide range of neural and behavioral data in cognitive science, including motor control, memory, visual perception, and development. A smaller amount of work has explored the suitability of DFT to account for natural language phenomena, such as in speech articulation and perception (e.g., Roon & Gafos, 2016; Stern & Shaw, 2023; Tilsen, 2019; Kirkham & Strycharczuk, 2024; Harper, 2021; Burroni, 2023; Jackson, Spencer, & Nam, 2016), and word learning and lexical semantics (e.g., Bhat et al., 2022; Sabinasz et al., 2023). 

This symposium will provide a general overview of DFT and showcase its applicability to a variety of speech and language phenomena: speech errors, language interference in code-switching, phonetic reduction, probabilistic phonotactics, and syntactic priming. We’ll show that this seemingly disparate set of facts can be understood via a common framework. In presenting this framework to the linguistics community, we hope to foster discussion about both the topics presented in the symposium, and, more broadly, the implications of this framework for understanding human language and cognition. As organizers, we hope to broaden our own understanding of the potential and pitfalls of DFT for language research. 

The symposium will begin with a general introduction on DFT followed by a question period (25 minutes). Five short talks using DFT to model linguistic phenomena (20 minutes each) will follow, intermixed with two commentary sessions led by a discussant (20 minutes each). We will also include a short break during the symposium to allow for continued conversations between members of the audience and presenters (15 minutes). During the discussion and question sessions, presenters will receive commentary on their presentations from both the discussant as well as members of the audience.

## Talks
**Discussant**: Khalil Iskarous

Michael Stern: *Dynamic Field Theory: An introduction*
- slides: `slides/Stern_DFT_intro.pdf`

Manasvi Chaturvedi: *Speech Errors in Vowels: trace effects*
- slides: `slides/Chaturvedi_VowelErrors.pdf`
- code: `MATLAB Scripts/Chaturvedi_vowel_errors_ae_a.m`

Alessandra Pintado-Urbanc: *Asymmetric Interference Effects in Code-Switching*
- slides: `slides/Pintado-Urbanc_Code-Switching.pdf`
- code: `MATLAB Scripts/Pintado-Urbanc_Code-Switching_Script.m`

Xiaomeng (Miranda) Zhu: *A Dynamic Neural Field Model for Production Mode and Phonological Neighborhood Density Effects*
- slides: `slides/Zhu_PND.pdf`
- code: `MATLAB Scripts/Zhu_PND_{Part1, Part2, Part3}.m`

Ayla Karakaş: *Deriving sibilant-vowel phonotactics from a soft bias in perception*
- slides: `slides/Karakaş_sibilant_vowel_phonotactics.pdf`
- code: `MATLAB Scripts/karakas_whalen_sim.m`

Zhenghao (Herbert) Zhou: *Error-driven Learning in DFT: A case study of structural priming*
- slides: `slides/LSA DFT Symposium - Error-driven Learning & Priming.pdf`
